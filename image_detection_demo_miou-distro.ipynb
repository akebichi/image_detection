{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import itertools\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.core.protobuf import saved_model_pb2\n",
    "from tensorflow.python.util import compat\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object detection imports\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.dataset_tools import create_coco_tf_record\n",
    "from object_detection.inference import deetection_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'faster_rcnn_resnet101_coco_2018_01_28'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "LOGDIR= MODEL_NAME + '/logs'\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading label map\n",
    "path=\"/home/rkebichi/Temp/models/research/object_detection\"\n",
    "os.chdir(path)\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper code\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "    (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Detection\n",
    "PATH_TO_TEST_IMAGES_DIR = \"/home/rkebichi/Temp/cocoapi/images/samples\"\n",
    "TEST_IMAGE_PATHS = [f for f in listdir(PATH_TO_TEST_IMAGES_DIR) if isfile(join(PATH_TO_TEST_IMAGES_DIR, f))]\n",
    "IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "     \n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})      \n",
    "      \n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image_path in TEST_IMAGE_PATHS:\n",
    "\n",
    "#if (len(TEST_IMAGE_PATHS) > 1):\n",
    "  #image_path = TEST_IMAGE_PATHS[0]\n",
    "  #image = Image.open(join(PATH_TO_TEST_IMAGES_DIR, image_path))\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(join(PATH_TO_TEST_IMAGES_DIR, image_path))\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict= run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  #print (num_detect)\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)\n",
    "  #print (output_dict['detection_boxes'])\n",
    "   #print (output_dict.get('detection_masks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "total_time = 0.0;\n",
    "grbboxes_filenames=[]\n",
    "img_bbox_ids=[]\n",
    "filename_img_bbox_ids=[]\n",
    "\n",
    "def gr_bbox():\n",
    "    json_filename=\"/home/rkebichi/Temp/cocoapi/annotations/instances_val2017.json\"\n",
    "    array=[]\n",
    "    try:\n",
    "        with open(json_filename) as data_file:\n",
    "            data=json.load(data_file)\n",
    "            img_height=0\n",
    "            img_width=0\n",
    "            for each_axis in data['images']:\n",
    "                img_height = each_axis['height']\n",
    "                img_width = each_axis['width']\n",
    "                img_filename = each_axis['file_name']\n",
    "                grbboxes_filenames.append(img_filename)\n",
    "                image_id = each_axis['id']\n",
    "                filename_img_bbox_ids.append(image_id)\n",
    "                \n",
    "            if len(grbboxes_filenames) != len(filename_img_bbox_ids):\n",
    "                print (\"these 2 should be the same length\")\n",
    "            \n",
    "            for each_axis in data['annotations']:\n",
    "                image_id =  each_axis['image_id']\n",
    "                img_bbox_ids.append(image_id)\n",
    "                X = each_axis['bbox']\n",
    "                if image_id == 285:\n",
    "                    #print (\"img id\", image_id)\n",
    "                    print (\"bbox\", X)\n",
    "                tmp_bb=[0, 0, 0, 0]\n",
    "                #tmp_bb[0]=X[2]/img_height\n",
    "                #tmp_bb[1]=X[3]/img_width\n",
    "                #tmp_bb[2]=X[0]/img_height\n",
    "                #tmp_bb[3]=X[1]/img_width\n",
    "                tmp_bb[0]=X[0]/img_width\n",
    "                tmp_bb[1]=X[1]/img_height\n",
    "                tmp_bb[2]=tmp_bb[0]+(X[2]/img_width)\n",
    "                tmp_bb[3]=tmp_bb[1]+(X[3]/img_height)\n",
    "                X=tmp_bb\n",
    "                if image_id == 285:\n",
    "                    print (\"bbox normalized\", X)\n",
    "                array.append(X)\n",
    "                \n",
    "            if len(img_bbox_ids) != len(array):\n",
    "                print (\"These 2 should also have same length\")\n",
    "                \n",
    "    except:\n",
    "        print(\"Unexpected error\", sys.exc_info()[0])\n",
    "        raise\n",
    "        \n",
    "    gr_bboxes = list()\n",
    "    #max_coord=np.max(array)\n",
    "    for bbox in array:\n",
    "        #print (bbox)\n",
    "        #bbox = [x/max_coord for x in bbox[:4]]\n",
    "        #bbox = [x for x in bbox[:4]]\n",
    "        #bbox = [x/256 for x in bbox[:4]]\n",
    "        gr_bboxes.append(bbox)\n",
    "        \n",
    "    return gr_bboxes\n",
    "        \n",
    "gr_bboxes = gr_bbox()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_img_id(img_filename):\n",
    "    countl=0\n",
    "    for filen in grbboxes_filenames:\n",
    "        if img_filename == filen:\n",
    "            return countl\n",
    "        countl = countl + 1\n",
    "        \n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def bb_IOU(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = np.maximum(xB - xA, 0) * np.maximum(yB - yA, 0)\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs(boxA[2] - boxA[0]) * abs(boxA[3] - boxA[1])\n",
    "    boxBArea = abs(boxB[2] - boxB[0]) * abs(boxB[3] - boxB[1])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def get_iou(pred_bboxes,gr_bboxes,img_id_to_check):\n",
    "    iou=0\n",
    "    \n",
    "    for pred_box in pred_bboxes:  \n",
    "        for gr_box in gr_bboxes: \n",
    "            #print (\"this is one pred box\")\n",
    "            #print (pred_box)\n",
    "            #print (\"this is one gr_box\")\n",
    "            #print (gr_box)                \n",
    "            iou = bb_IOU(pred_box, gr_box)  \n",
    "            if iou > 0.5:\n",
    "                print (\"IOU is\")\n",
    "                print (iou)\n",
    "                return iou\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def get_r_boxes_single_imag(image_path,img_id):\n",
    "     \n",
    "    \"\"\"Function to get the Ground truth boxes for a specific image\n",
    "\n",
    "     Args:\n",
    "     image_path: path for the image to get the ground truth boxes for\n",
    "     img_id    : Image ID, incase it's already stored in the system\n",
    "\n",
    "     Return\n",
    "    gr_bboxes_update: truth boxes and their co-ordinates (normalized)\n",
    "    \n",
    "    \"\"\"\n",
    "    #json_filename=\"/home/rkebichi/Temp/cocoapi/annotations/instances_val2017.json\"\n",
    "    array=[]\n",
    "    #try:\n",
    "     #   with open(json_filename) as data_file:\n",
    "      #      data=json.load(data_file)\n",
    "    img_height=0\n",
    "    img_width=0\n",
    "    for each_axis in data['images']:\n",
    "        if image_path == each_axis['file_name']:\n",
    "             # Get the Images dimension\n",
    "                    img_height = each_axis['height']\n",
    "                    img_width = each_axis['width']\n",
    "                    imge_id = each_axis['id']\n",
    "   \n",
    "    for each_axis in data['annotations']:\n",
    "                if imge_id == each_axis['image_id']:\n",
    "                    X = each_axis['bbox']\n",
    "                    \n",
    "                    tmp_bb=[0, 0, 0, 0]\n",
    "                    tmp_bb[0]=X[0]/img_width\n",
    "                    tmp_bb[1]=X[1]/img_height\n",
    "                    tmp_bb[2]=tmp_bb[0]+X[2]/img_width\n",
    "                    tmp_bb[3]=tmp_bb[1]+X[3]/img_height\n",
    "                    X=[tmp_bb[1],tmp_bb[0],tmp_bb[3],tmp_bb[2]]\n",
    "                    \n",
    "                    array.append(X)\n",
    "                    \n",
    "  #  except:\n",
    "   #     print(\"Unexpected error\", sys.exc_info()[0])\n",
    "    #    raise              \n",
    "\n",
    "        \n",
    "    gr_bboxes_update = list()\n",
    "    \n",
    "    for bbox in array:\n",
    "         gr_bboxes_update.append(bbox)\n",
    "   \n",
    "    \n",
    "\n",
    "    return gr_bboxes_update\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_out = list()\n",
    "pred_bboxes = 0\n",
    "gr_found=0\n",
    "count2=0\n",
    "\n",
    "json_filename=\"/home/rkebichi/Temp/cocoapi/annotations/instances_val2017.json\"\n",
    "try:\n",
    "        with open(json_filename) as data_file:\n",
    "            data=json.load(data_file)\n",
    "except:\n",
    "            print(\"Unexpected error\", sys.exc_info()[0])\n",
    "            raise\n",
    "#mylp = [TEST_IMAGE_PATHS[0]]\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "#for image_path in mylp:\n",
    "    image = Image.open(join(PATH_TO_TEST_IMAGES_DIR, image_path)) \n",
    "    print (\"Processing image \")\n",
    "    print (image_path)\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    t1 = time.time()\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    pred_bboxes += output_dict['num_detections']\n",
    "    \n",
    "     \n",
    "    loc=find_img_id(image_path)\n",
    "    if loc==-1:\n",
    "        print (\"index was found to be -1 , something wrong\")\n",
    "    print (\"Processing img with ID\")\n",
    "    img_id_to_check=filename_img_bbox_ids[loc]\n",
    "    print (img_id_to_check)\n",
    "    #get the Ground truth boxes for this image\n",
    "    gr_boxes=get_r_boxes_single_imag(image_path,img_id_to_check)\n",
    "    gr_found+=len(gr_boxes)       \n",
    "    for mybbox in output_dict['detection_boxes']:\n",
    "        \n",
    "            list1 = []\n",
    "            list1.append(mybbox)\n",
    "            iou=get_iou(list1, gr_boxes,img_id_to_check)\n",
    "            if iou > 0.5: # count any coverage\n",
    "                iou_out.append(iou)\n",
    "            \n",
    "            t2 = time.time()\n",
    "            #print(\"time \", t2 - t1)\n",
    "            total_time += (t2 - t1)\n",
    "            count2 = count2 + 1\n",
    "        \n",
    "    print('mean time=')\n",
    "    print(total_time/count2)\n",
    "    \n",
    "MIOU=sum(iou_out)/len(iou_out)*100\n",
    "precision=gr_found/pred_bboxes\n",
    "print(\"MIOU is\")\n",
    "print(MIOU)\n",
    "print('*'*40)\n",
    "print (\"Model was right that many times\")\n",
    "print(len(iou_out))\n",
    "print (\"Total predicted boxes\")\n",
    "print (pred_bboxes)\n",
    "print (\"Total gt boxes\")\n",
    "print(gr_found)\n",
    "print(\"The precision score is:\")\n",
    "print(precision)\n",
    "print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the data input format from images and annonation (json) to TF records format\n",
    "# path to import the jason file\n",
    "json_filename=\"/home/rkebichi/Temp/cocoapi/annotations/instances_val2017.json\"\n",
    "#Path to where the Tf files will be stored\n",
    "output_path=\"/home/rkebichi/Temp/cocoapi/annotations/tfrecord/out.record\"\n",
    "#Path to the image directory\n",
    "imag_dir=\"/home/rkebichi/Temp/cocoapi/images/val2017\"\n",
    "#Num of output files (will keep it 1 for now)\n",
    "num_data_files=1\n",
    "create_coco_tf_record._create_tf_record_from_coco_annotations(json_filename, imag_dir, output_path,False,num_data_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the input from tf_records \n",
    "input_tfrecord_paths=\"/home/rkebichi/Temp/cocoapi/annotations/tfrecord/out.record-00000-of-00001\"\n",
    "input_tfrecord_paths = [v for v in input_tfrecord_paths.split(',') if v]\n",
    "tf.logging.info('Reading input from %d files', len(input_tfrecord_paths))\n",
    "serialized_example_tensor, image_tensor = deetection_inference.build_input(input_tfrecord_paths)\n",
    "detected_boxes_tensor, detected_scores_tensor, detected_labels_tensor=deetection_inference.build_inference_graph(image_tensor, od_graph_def)\n",
    "\n",
    "\n",
    "#Runs the supplied tensors and adds the inferred detections to the example.\n",
    "tf_example= deetection_inference.infer_detections_and_add_to_example(serialized_example_tensor, detected_boxes_tensor, detected_scores_tensor,detected_labels_tensor,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
